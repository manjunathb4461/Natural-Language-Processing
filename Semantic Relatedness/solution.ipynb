{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT  2\n",
    "## Team Number : 10\n",
    "\n",
    "### Team Members :\n",
    "\n",
    "   #### Manjunath Bhadrannavar : 01FB16ECS195\n",
    "   #### Megha Kalal : 01FB16ECS203\n",
    "   #### Nikhil V Revankar : 01FB16ECS230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents1 = [\"I like fish fry with lunch whenever I am by the side of sea.\",\n",
    "          \"He is visiting bank to deposit all his cash earned during the day It will earn interest overtime.\",\n",
    "          \"Pure malt whiskey with a bit of soda can make you do what you are afraid of doing. It can make you brave for a while.\",\n",
    "          \"It is a horse that is more faithful and powerful than my car. It loves me, cares for me, talks to me and makes me feel good when I am upset.\",\n",
    "          \"I have a pen that writes like a dream. I carry it in all my examinations as the examiners need good hand-writing on the answer scripts.\",\n",
    "          \"My driving license is my identity in USA as everything is linked to it.\"]\n",
    "\n",
    "sents2 = [\"Our professor  is ferocious i.e. if you make noise in classroom, he will fry you in front of everybody\",\n",
    "          \"During flood, better not go to the river bank though they are depositing stones to harden it.\",\n",
    "          \"They asked me to develop a machine learning algorithm that I was so afraid of doing. I ended up looking for a job.\",\n",
    "          \"My old car is the best i.e. faithful, smooth, powerful and has never stalled on the highway.\",\n",
    "          \"I had a dream that I was having a holiday in this island with the sea gulls flying by and sea breeze blowing.\",\n",
    "          \"He is the strongest leader we have seen in this island nation and he gave us identity in the world.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text segment 1 chosen for experiment\n",
    "\n",
    "text1 = sents1[0]\n",
    "text2 = sents2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [text1,text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic vector of the corpus\n",
    "def preprocess(text_list):\n",
    "    stop_words = list(set(nltk.corpus.stopwords.words('english')))\n",
    "    stop_words.extend(['(', ')', '[', ']', '{', '}', '.', ',', '!', '*','i.e'])\n",
    "    stop_words.extend([chr(i) for i in range(97,123)])\n",
    "    my_corpus = []\n",
    "    ind_corpus = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for text in text_list:\n",
    "        #tokenized_text = nltk.word_tokenize(text)\n",
    "        text = text.lower()\n",
    "        tokenized_text =tokenizer.tokenize(text)\n",
    "        for word in stop_words:\n",
    "            tokenized_text = list(filter(lambda a: a != word, tokenized_text))\n",
    "        my_corpus.extend(tokenized_text)\n",
    "        ind_corpus.append(tokenized_text)\n",
    "        \n",
    "    my_corpus = list(set(my_corpus))\n",
    "    #print(my_corpus)\n",
    "    return my_corpus,ind_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using spacy\n",
    "# Similarity matrix using spaCy\n",
    "def spacy_sim_matrix(vector):\n",
    "    tokens = nlp(' '.join(vector))\n",
    "    sim_mat = []\n",
    "    for tok in tokens:\n",
    "        sim_mat.append([tok.similarity(other_tok) for other_tok in tokens])\n",
    "    sim_mat = pd.DataFrame(sim_mat, index=vector, columns=vector)                                 \n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               make     noise     lunch     front  classroom  everybody  \\\n",
      "make       1.000000  0.239830  0.224730  0.125662   0.291178   0.206717   \n",
      "noise      0.239830  1.000000  0.541300  0.494886   0.514236   0.440464   \n",
      "lunch      0.224730  0.541300  1.000000  0.576543   0.446029   0.350097   \n",
      "front      0.125662  0.494886  0.576543  1.000000   0.275211   0.256349   \n",
      "classroom  0.291178  0.514236  0.446029  0.275211   1.000000   0.512594   \n",
      "everybody  0.206717  0.440464  0.350097  0.256349   0.512594   1.000000   \n",
      "fry        0.414048  0.349236  0.402552  0.480363   0.342750   0.387895   \n",
      "professor  0.341659  0.628171  0.585894  0.430072   0.697117   0.499030   \n",
      "sea        0.218223  0.510816  0.463107  0.475711   0.469951   0.495240   \n",
      "fish       0.199333  0.487422  0.399099  0.388916   0.578912   0.505522   \n",
      "side       0.251184  0.387571  0.340186  0.403261   0.453875   0.510754   \n",
      "ferocious  0.144574  0.265068  0.262756  0.299164   0.225927   0.221581   \n",
      "like       0.195781  0.046092  0.268655  0.128027   0.055346   0.143471   \n",
      "whenever  -0.272905  0.047486  0.089661 -0.012476   0.022108   0.059364   \n",
      "\n",
      "                fry  professor       sea      fish      side  ferocious  \\\n",
      "make       0.414048   0.341659  0.218223  0.199333  0.251184   0.144574   \n",
      "noise      0.349236   0.628171  0.510816  0.487422  0.387571   0.265068   \n",
      "lunch      0.402552   0.585894  0.463107  0.399099  0.340186   0.262756   \n",
      "front      0.480363   0.430072  0.475711  0.388916  0.403261   0.299164   \n",
      "classroom  0.342750   0.697117  0.469951  0.578912  0.453875   0.225927   \n",
      "everybody  0.387895   0.499030  0.495240  0.505522  0.510754   0.221581   \n",
      "fry        1.000000   0.522282  0.376118  0.467270  0.538475   0.444467   \n",
      "professor  0.522282   1.000000  0.580925  0.564255  0.481520   0.296985   \n",
      "sea        0.376118   0.580925  1.000000  0.622023  0.322973   0.137785   \n",
      "fish       0.467270   0.564255  0.622023  1.000000  0.543977   0.328011   \n",
      "side       0.538475   0.481520  0.322973  0.543977  1.000000   0.435354   \n",
      "ferocious  0.444467   0.296985  0.137785  0.328011  0.435354   1.000000   \n",
      "like       0.237583   0.117911  0.047148  0.061024  0.191792   0.235827   \n",
      "whenever  -0.081063   0.069457  0.083593  0.047982  0.019324  -0.011562   \n",
      "\n",
      "               like  whenever  \n",
      "make       0.195781 -0.272905  \n",
      "noise      0.046092  0.047486  \n",
      "lunch      0.268655  0.089661  \n",
      "front      0.128027 -0.012476  \n",
      "classroom  0.055346  0.022108  \n",
      "everybody  0.143471  0.059364  \n",
      "fry        0.237583 -0.081063  \n",
      "professor  0.117911  0.069457  \n",
      "sea        0.047148  0.083593  \n",
      "fish       0.061024  0.047982  \n",
      "side       0.191792  0.019324  \n",
      "ferocious  0.235827 -0.011562  \n",
      "like       1.000000  0.087344  \n",
      "whenever   0.087344  1.000000  \n"
     ]
    }
   ],
   "source": [
    "my_corpus,ind_corpus= preprocess(text_list)\n",
    "similarity_matrix = spacy_sim_matrix(my_corpus)\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART-1\n",
    "#using nltk\n",
    "\n",
    "# semantic relatedness between words\n",
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "\n",
    "    \n",
    "def sem_relatedness_word(method,vector):\n",
    "    entities = [wn.synsets(word)[0] for word in vector if wn.synsets(word)!=[] and wn.synsets(word)[0].pos() != 's']\n",
    "    entity_names = [entity.name().split('.')[0] for entity in entities]\n",
    "    similarities = []\n",
    "    \n",
    "    # Path Based Similarity\n",
    "    if method == \"path\":\n",
    "        similarities = [[entity.path_similarity(compared_entity) for compared_entity in entities] for entity in entities]\n",
    "        \n",
    "    elif method == 'lch':\n",
    "        similarities = [[entity.lch_similarity(compared_entity) if entity.pos()==compared_entity.pos() else None for compared_entity in entities]\n",
    "                        for entity in entities]\n",
    "        \n",
    "    elif method == 'wup':\n",
    "        similarities = [[entity.wup_similarity(compared_entity) if  entity.pos()==compared_entity.pos() else None\n",
    "                         for compared_entity in entities] for entity in entities]\n",
    "    \n",
    "    # Distribution Based Similarity\n",
    "    elif method == 'res':\n",
    "        similarities = [[entity.res_similarity(compared_entity, ic=brown_ic) if entity.pos()==compared_entity.pos() else None \n",
    "                         for compared_entity in entities] for entity in entities]\n",
    "    \n",
    "    elif method == 'jcn':\n",
    "        similarities = [[entity.jcn_similarity(compared_entity, ic=brown_ic) if entity.pos()==compared_entity.pos() else None \n",
    "                         for compared_entity in entities] for entity in entities]\n",
    "        \n",
    "    elif method == 'lin':\n",
    "        similarities = [[entity.lin_similarity(compared_entity, ic=brown_ic) if entity.pos()==compared_entity.pos() else None \n",
    "                         for compared_entity in entities] for entity in entities]\n",
    "        \n",
    "    similarity_frame = pd.DataFrame(similarities,\n",
    "                                    index=entity_names, \n",
    "                                    columns=entity_names)\n",
    "    similarity_frame.fillna(0,inplace=True)               \n",
    "    print(similarity_frame)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              brand     noise     lunch     front  classroom       fry  \\\n",
      "brand      1.000000  0.083333  0.058824  0.052632   0.055556  0.058824   \n",
      "noise      0.083333  1.000000  0.071429  0.062500   0.066667  0.071429   \n",
      "lunch      0.058824  0.071429  1.000000  0.066667   0.071429  0.076923   \n",
      "front      0.052632  0.062500  0.066667  1.000000   0.071429  0.066667   \n",
      "classroom  0.055556  0.066667  0.071429  0.071429   1.000000  0.076923   \n",
      "fry        0.058824  0.071429  0.076923  0.066667   0.076923  1.000000   \n",
      "professor  0.055556  0.066667  0.071429  0.062500   0.071429  0.142857   \n",
      "sea        0.071429  0.090909  0.100000  0.083333   0.090909  0.100000   \n",
      "fish       0.050000  0.058824  0.062500  0.062500   0.076923  0.090909   \n",
      "side       0.066667  0.083333  0.090909  0.142857   0.100000  0.090909   \n",
      "like       0.333333  0.083333  0.058824  0.052632   0.055556  0.058824   \n",
      "\n",
      "           professor       sea      fish      side      like  \n",
      "brand       0.055556  0.071429  0.050000  0.066667  0.333333  \n",
      "noise       0.066667  0.090909  0.058824  0.083333  0.083333  \n",
      "lunch       0.071429  0.100000  0.062500  0.090909  0.058824  \n",
      "front       0.062500  0.083333  0.062500  0.142857  0.052632  \n",
      "classroom   0.071429  0.090909  0.076923  0.100000  0.055556  \n",
      "fry         0.142857  0.100000  0.090909  0.090909  0.058824  \n",
      "professor   1.000000  0.090909  0.083333  0.083333  0.055556  \n",
      "sea         0.090909  1.000000  0.076923  0.125000  0.071429  \n",
      "fish        0.083333  0.076923  1.000000  0.083333  0.050000  \n",
      "side        0.083333  0.125000  0.083333  1.000000  0.066667  \n",
      "like        0.055556  0.071429  0.050000  0.066667  1.000000  \n"
     ]
    }
   ],
   "source": [
    "#path_similarity\n",
    "sem_relatedness_word('path',my_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              brand     noise     lunch     front  classroom       fry  \\\n",
      "brand      3.637586  1.152680  0.804373  0.693147   0.747214  0.804373   \n",
      "noise      1.152680  3.637586  0.998529  0.864997   0.929536  0.998529   \n",
      "lunch      0.804373  0.998529  3.637586  0.929536   0.998529  1.072637   \n",
      "front      0.693147  0.864997  0.929536  3.637586   0.998529  0.929536   \n",
      "classroom  0.747214  0.929536  0.998529  0.998529   3.637586  1.072637   \n",
      "fry        0.804373  0.998529  1.072637  0.929536   1.072637  3.637586   \n",
      "professor  0.747214  0.929536  0.998529  0.864997   0.998529  1.691676   \n",
      "sea        0.998529  1.239691  1.335001  1.152680   1.239691  1.335001   \n",
      "fish       0.641854  0.804373  0.864997  0.864997   1.072637  1.239691   \n",
      "side       0.929536  1.152680  1.239691  1.691676   1.335001  1.239691   \n",
      "like       2.538974  1.152680  0.804373  0.693147   0.747214  0.804373   \n",
      "\n",
      "           professor       sea      fish      side      like  \n",
      "brand       0.747214  0.998529  0.641854  0.929536  2.538974  \n",
      "noise       0.929536  1.239691  0.804373  1.152680  1.152680  \n",
      "lunch       0.998529  1.335001  0.864997  1.239691  0.804373  \n",
      "front       0.864997  1.152680  0.864997  1.691676  0.693147  \n",
      "classroom   0.998529  1.239691  1.072637  1.335001  0.747214  \n",
      "fry         1.691676  1.335001  1.239691  1.239691  0.804373  \n",
      "professor   3.637586  1.239691  1.152680  1.152680  0.747214  \n",
      "sea         1.239691  3.637586  1.072637  1.558145  0.998529  \n",
      "fish        1.152680  1.072637  3.637586  1.152680  0.641854  \n",
      "side        1.152680  1.558145  1.152680  3.637586  0.929536  \n",
      "like        0.747214  0.998529  0.641854  0.929536  3.637586  \n"
     ]
    }
   ],
   "source": [
    "#lch_similarity\n",
    "sem_relatedness_word('lch',my_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              brand     noise     lunch     front  classroom       fry  \\\n",
      "brand      1.000000  0.352941  0.111111  0.100000   0.105263  0.111111   \n",
      "noise      0.352941  1.000000  0.133333  0.117647   0.125000  0.133333   \n",
      "lunch      0.111111  0.133333  1.000000  0.222222   0.235294  0.250000   \n",
      "front      0.100000  0.117647  0.222222  1.000000   0.315789  0.300000   \n",
      "classroom  0.105263  0.125000  0.235294  0.315789   1.000000  0.400000   \n",
      "fry        0.111111  0.133333  0.250000  0.300000   0.400000  1.000000   \n",
      "professor  0.105263  0.125000  0.235294  0.285714   0.380952  0.521739   \n",
      "sea        0.133333  0.166667  0.307692  0.266667   0.285714  0.307692   \n",
      "fish       0.095238  0.111111  0.210526  0.285714   0.400000  0.545455   \n",
      "side       0.125000  0.153846  0.285714  0.625000   0.400000  0.375000   \n",
      "like       0.900000  0.352941  0.111111  0.100000   0.105263  0.111111   \n",
      "\n",
      "           professor       sea      fish      side      like  \n",
      "brand       0.105263  0.133333  0.095238  0.125000  0.900000  \n",
      "noise       0.125000  0.166667  0.111111  0.153846  0.352941  \n",
      "lunch       0.235294  0.307692  0.210526  0.285714  0.111111  \n",
      "front       0.285714  0.266667  0.285714  0.625000  0.100000  \n",
      "classroom   0.380952  0.285714  0.400000  0.400000  0.105263  \n",
      "fry         0.521739  0.307692  0.545455  0.375000  0.111111  \n",
      "professor   1.000000  0.285714  0.521739  0.352941  0.105263  \n",
      "sea         0.285714  1.000000  0.250000  0.363636  0.133333  \n",
      "fish        0.521739  0.250000  1.000000  0.352941  0.095238  \n",
      "side        0.352941  0.363636  0.352941  1.000000  0.125000  \n",
      "like        0.105263  0.133333  0.095238  0.125000  1.000000  \n"
     ]
    }
   ],
   "source": [
    "#wup_similarity\n",
    "sem_relatedness_word('wup',my_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              brand     noise      lunch     front  classroom        fry  \\\n",
      "brand      7.732198  1.592755  -0.000000 -0.000000  -0.000000  -0.000000   \n",
      "noise      1.592755  7.741767  -0.000000 -0.000000  -0.000000  -0.000000   \n",
      "lunch     -0.000000 -0.000000  10.371255  0.801759   0.801759   0.801759   \n",
      "front     -0.000000 -0.000000   0.801759  8.137663   1.290026   1.290026   \n",
      "classroom -0.000000 -0.000000   0.801759  1.290026  11.169763   1.531834   \n",
      "fry       -0.000000 -0.000000   0.801759  1.290026   1.531834  13.772453   \n",
      "professor -0.000000 -0.000000   0.801759  1.290026   1.531834   6.394069   \n",
      "sea       -0.000000 -0.000000   0.801759  0.801759   0.801759   0.801759   \n",
      "fish      -0.000000 -0.000000   0.801759  1.290026   1.531834   2.224150   \n",
      "side      -0.000000 -0.000000   0.801759  4.944545   1.290026   1.290026   \n",
      "like       6.421295  1.592755  -0.000000 -0.000000  -0.000000  -0.000000   \n",
      "\n",
      "           professor       sea      fish      side       like  \n",
      "brand      -0.000000 -0.000000 -0.000000 -0.000000   6.421295  \n",
      "noise      -0.000000 -0.000000 -0.000000 -0.000000   1.592755  \n",
      "lunch       0.801759  0.801759  0.801759  0.801759  -0.000000  \n",
      "front       1.290026  0.801759  1.290026  4.944545  -0.000000  \n",
      "classroom   1.531834  0.801759  1.531834  1.290026  -0.000000  \n",
      "fry         6.394069  0.801759  2.224150  1.290026  -0.000000  \n",
      "professor  10.121794  0.801759  2.224150  1.290026  -0.000000  \n",
      "sea         0.801759  9.502755  0.801759  0.801759  -0.000000  \n",
      "fish        2.224150  0.801759  8.116461  1.290026  -0.000000  \n",
      "side        1.290026  0.801759  1.290026  7.559847  -0.000000  \n",
      "like       -0.000000 -0.000000 -0.000000 -0.000000  10.999864  \n"
     ]
    }
   ],
   "source": [
    "#res_similarity\n",
    "sem_relatedness_word('res',my_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   brand          noise          lunch          front  \\\n",
      "brand      1.000000e+300   8.137719e-02   5.523808e-02   6.301252e-02   \n",
      "noise       8.137719e-02  1.000000e+300   5.520890e-02   6.297455e-02   \n",
      "lunch       5.523808e-02   5.520890e-02  1.000000e+300   5.915270e-02   \n",
      "front       6.301252e-02   6.297455e-02   5.915270e-02  1.000000e+300   \n",
      "classroom   5.290456e-02   5.287779e-02   5.015674e-02   5.978224e-02   \n",
      "fry         4.650157e-02   4.648089e-02   4.436520e-02   5.173288e-02   \n",
      "professor   5.600988e-02   5.597988e-02   5.293938e-02   6.377793e-02   \n",
      "sea         5.802163e-02   5.798943e-02   5.473306e-02   6.235619e-02   \n",
      "fish        6.309682e-02   6.305875e-02   5.922698e-02   7.313110e-02   \n",
      "side        6.539348e-02   6.535258e-02   6.124605e-02   1.721639e-01   \n",
      "like        1.697945e-01   6.428337e-02   4.679212e-02   5.225336e-02   \n",
      "\n",
      "               classroom            fry      professor            sea  \\\n",
      "brand       5.290456e-02   4.650157e-02   5.600988e-02   5.802163e-02   \n",
      "noise       5.287779e-02   4.648089e-02   5.597988e-02   5.798943e-02   \n",
      "lunch       5.015674e-02   4.436520e-02   5.293938e-02   5.473306e-02   \n",
      "front       5.978224e-02   5.173288e-02   6.377793e-02   6.235619e-02   \n",
      "classroom  1.000000e+300   4.570687e-02   5.486098e-02   5.244113e-02   \n",
      "fry         4.570687e-02  1.000000e+300   9.004053e-02   4.614315e-02   \n",
      "professor   5.486098e-02   9.004053e-02  1.000000e+300   5.549072e-02   \n",
      "sea         5.244113e-02   4.614315e-02   5.549072e-02  1.000000e+300   \n",
      "fish        6.164257e-02   5.733744e-02   7.251656e-02   6.243874e-02   \n",
      "side        6.192120e-02   5.332694e-02   6.621819e-02   6.468689e-02   \n",
      "like        4.510676e-02   4.036764e-02   4.734477e-02   4.877426e-02   \n",
      "\n",
      "                    fish           side           like  \n",
      "brand       6.309682e-02   6.539348e-02   1.697945e-01  \n",
      "noise       6.305875e-02   6.535258e-02   6.428337e-02  \n",
      "lunch       5.922698e-02   6.124605e-02   4.679212e-02  \n",
      "front       7.313110e-02   1.721639e-01   5.225336e-02  \n",
      "classroom   6.164257e-02   6.192120e-02   4.510676e-02  \n",
      "fry         5.733744e-02   5.332694e-02   4.036764e-02  \n",
      "professor   7.251656e-02   6.621819e-02   4.734477e-02  \n",
      "sea         6.243874e-02   6.468689e-02   4.877426e-02  \n",
      "fish       1.000000e+300   7.635770e-02   5.231131e-02  \n",
      "side        7.635770e-02  1.000000e+300   5.388015e-02  \n",
      "like        5.231131e-02   5.388015e-02  1.000000e+300  \n"
     ]
    }
   ],
   "source": [
    "#jcn_similarity\n",
    "sem_relatedness_word('jcn',my_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              brand     noise     lunch     front  classroom       fry  \\\n",
      "brand      1.000000  0.205863 -0.000000 -0.000000  -0.000000 -0.000000   \n",
      "noise      0.205863  1.000000 -0.000000 -0.000000  -0.000000 -0.000000   \n",
      "lunch     -0.000000 -0.000000  1.000000  0.086635   0.074440  0.066416   \n",
      "front     -0.000000 -0.000000  0.086635  1.000000   0.133630  0.117756   \n",
      "classroom -0.000000 -0.000000  0.074440  0.133630   1.000000  0.122831   \n",
      "fry       -0.000000 -0.000000  0.066416  0.117756   0.122831  1.000000   \n",
      "professor -0.000000 -0.000000  0.078247  0.141299   0.143891  0.535197   \n",
      "sea       -0.000000 -0.000000  0.080684  0.090900   0.077568  0.068894   \n",
      "fish      -0.000000 -0.000000  0.086734  0.158732   0.158853  0.203222   \n",
      "side      -0.000000 -0.000000  0.089427  0.629978   0.137753  0.120946   \n",
      "like       0.685594  0.169970 -0.000000 -0.000000  -0.000000 -0.000000   \n",
      "\n",
      "           professor       sea      fish      side      like  \n",
      "brand      -0.000000 -0.000000 -0.000000 -0.000000  0.685594  \n",
      "noise      -0.000000 -0.000000 -0.000000 -0.000000  0.169970  \n",
      "lunch       0.078247  0.080684  0.086734  0.089427 -0.000000  \n",
      "front       0.141299  0.090900  0.158732  0.629978 -0.000000  \n",
      "classroom   0.143891  0.077568  0.158853  0.137753 -0.000000  \n",
      "fry         0.535197  0.068894  0.203222  0.120946 -0.000000  \n",
      "professor   1.000000  0.081710  0.243899  0.145917 -0.000000  \n",
      "sea         0.081710  1.000000  0.091010  0.093979 -0.000000  \n",
      "fish        0.243899  0.091010  1.000000  0.164583 -0.000000  \n",
      "side        0.145917  0.093979  0.164583  1.000000 -0.000000  \n",
      "like       -0.000000 -0.000000 -0.000000 -0.000000  1.000000  \n"
     ]
    }
   ],
   "source": [
    "#lin_similarity\n",
    "sem_relatedness_word('lin',my_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic vector representing text segments\n",
    "def sem_vector(my_corpus, ind_corpus):\n",
    "    sim_mat = spacy_sim_matrix(my_corpus)\n",
    "    threshold = max(sim_mat.mean())\n",
    "    v = [[], []]\n",
    "    \n",
    "    for i in range(2):\n",
    "        for word in my_corpus:\n",
    "            if word in ind_corpus[i]:\n",
    "                v[i].append(1)\n",
    "            else:\n",
    "                max_sim = 0\n",
    "                for j in ind_corpus[i]:\n",
    "                    sim = nlp(word).similarity(nlp(j))\n",
    "                    if sim > max_sim:\n",
    "                        max_sim = sim\n",
    "                        sim_word = j\n",
    "                if max_sim > threshold:\n",
    "                    v[i].append(max_sim)\n",
    "                else:\n",
    "                    v[i].append(0)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic relatedness between sentences 1 : 0.8464173143763345 \n",
      "\n",
      "Semantic relatedness between sentences 2 : 0.8475331068550017 \n",
      "\n",
      "Semantic relatedness between sentences 3 : 0.9382694389292592 \n",
      "\n",
      "Semantic relatedness between sentences 4 : 0.8332144818258096 \n",
      "\n",
      "Semantic relatedness between sentences 5 : 0.9192209122283185 \n",
      "\n",
      "Semantic relatedness between sentences 6 : 0.8961191566636407 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#semantic relatedness between text segements\n",
    "\n",
    "for i, sent1, sent2 in zip([i+1 for i in range(len(sents1))], sents1, sents2):\n",
    "    p = preprocess([sent1, sent2])[1]\n",
    "    v1,v2 = sem_vector(list(set(p[0] + p[1])),[p[0], p[1]])\n",
    "    print(\"Semantic relatedness between sentences \"+str(i)+\" :\", cosine_similarity([v1],[v2])[0][0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word sense disambiguation\n",
    "from nltk.corpus import wordnet as wn, wordnet_ic\n",
    "from nltk.wsd import lesk\n",
    "from nltk import word_tokenize\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "\n",
    "def after_wsd_similarity(sample1, sample2):\n",
    "    #sentences = [sample1, sample2]\n",
    "\n",
    "    similar_words = [i for i in word_tokenize(sample1.lower()) for j in word_tokenize(sample2.lower()) if i==j]\n",
    "    similar_words = list(set([i for i in similar_words if i not in ['.', ',']]))\n",
    "\n",
    "    print('SENTENCES:', '\\n', sample1,'\\n', sample2)\n",
    "    print(\"COMMON_LEMMA :\", similar_words)\n",
    "    similarity, n = 0, 0\n",
    "    senses = []    \n",
    "    \n",
    "    for word in similar_words:\n",
    "        senses = []\n",
    "        for sentence in [sample1, sample2]:\n",
    "            word_syn = lesk(word_tokenize(sentence.lower()), word)\n",
    "            #print ('Word synset:', word_syn)\n",
    "            senses.append((word_syn, word_syn.name().split('.')[1]))\n",
    "\n",
    "        if senses[0][1] == senses[1][1]:\n",
    "            similarity += senses[0][0].path_similarity(senses[1][0],semcor_ic)\n",
    "            n += 1\n",
    "    if n != 0:\n",
    "        return (similarity/n)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCES: \n",
      " like fish fry lunch whenever side sea \n",
      " professor ferocious make noise classroom fry front everybody\n",
      "COMMON_LEMMA : ['fry']\n",
      "SIM. AFTER WSD : 1.0\n",
      "SIM. BEFORE WSD : 0.8464173143763345\n",
      "SIM. WITH ENSEMBLE : 0.8924921200634341 \n",
      "\n",
      "SENTENCES: \n",
      " visiting bank deposit cash earned day earn interest overtime \n",
      " flood better go river bank though depositing stones harden\n",
      "COMMON_LEMMA : ['bank']\n",
      "SIM. AFTER WSD : 1.0\n",
      "SIM. BEFORE WSD : 0.8475331068550017\n",
      "SIM. WITH ENSEMBLE : 0.8932731747985012 \n",
      "\n",
      "SENTENCES: \n",
      " pure malt whiskey bit soda make afraid make brave \n",
      " asked develop machine learning algorithm afraid ended looking job\n",
      "COMMON_LEMMA : ['afraid']\n",
      "SIM. AFTER WSD : 1.0\n",
      "SIM. BEFORE WSD : 0.9382694389292592\n",
      "SIM. WITH ENSEMBLE : 0.9567886072504814 \n",
      "\n",
      "SENTENCES: \n",
      " horse faithful powerful car loves cares talks makes feel good upset \n",
      " old car best faithful smooth powerful never stalled highway\n",
      "COMMON_LEMMA : ['car', 'faithful', 'powerful']\n",
      "SIM. AFTER WSD : 1.0\n",
      "SIM. BEFORE WSD : 0.8332144818258096\n",
      "SIM. WITH ENSEMBLE : 0.8832501372780666 \n",
      "\n",
      "SENTENCES: \n",
      " pen writes like dream carry examinations examiners need good hand writing answer scripts \n",
      " dream holiday island sea gulls flying sea breeze blowing\n",
      "COMMON_LEMMA : ['dream']\n",
      "SIM. AFTER WSD : 1.0\n",
      "SIM. BEFORE WSD : 0.9192209122283185\n",
      "SIM. WITH ENSEMBLE : 0.9434546385598228 \n",
      "\n",
      "SENTENCES: \n",
      " driving license identity usa everything linked \n",
      " strongest leader seen island nation gave us identity world\n",
      "COMMON_LEMMA : ['identity']\n",
      "SIM. AFTER WSD : 1.0\n",
      "SIM. BEFORE WSD : 0.8961191566636407\n",
      "SIM. WITH ENSEMBLE : 0.9272834096645484 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent1, sent2 in zip(sents1, sents2):\n",
    "    p = preprocess([sent1, sent2])[1]\n",
    "    after = after_wsd_similarity(\" \".join(p[0]), \" \".join(p[1]))\n",
    "    print('SIM. AFTER WSD :', after)\n",
    "    \n",
    "    v1,v2 = sem_vector(list(set(p[0] + p[1])),[p[0], p[1]])\n",
    "    before = cosine_similarity([v1],[v2])[0][0]\n",
    "    print(\"SIM. BEFORE WSD :\", before)\n",
    "    \n",
    "    ensemble = after * 0.3 + before * 0.7\n",
    "    print(\"SIM. WITH ENSEMBLE :\", ensemble, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step - 4 observations\n",
    "\n",
    "Comparing the semantic relatedness before and after wsd, \n",
    "we get to the fact that the common lemma occuring in both the sentence segments with same POS tags\n",
    "always have the same senses. This can be observed from the values achieved above.\n",
    "\n",
    "Semantic relatedness after wsd turned out to be 100% equal, meaning that the common lemma with same POS tags\n",
    "are not the differentiating words between the sentences above, instead all the words in the sentence matter\n",
    "when we are finding the semantic relatedness\n",
    "\n",
    "So, when finding the ensemble values for semantic relatedness, more weightage was given the method without wsd,\n",
    "causing it to have more effect on the result than the method with wsd\n",
    "\n",
    "This can vary from dataset to dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP-5\n",
    "## Decisions\n",
    "\n",
    "Coming to Path Based Similarity:\n",
    "    \n",
    "    WUP similarity seems to be better as compared to path_similarity and lch_similarity as WUP calculates the \n",
    "    relatedness by looking at the depth of the two synsets in the WordNet. The score can never be 0 as the depth of the \n",
    "    LCS (Least Common Subsumer) is never 0.\n",
    "    \n",
    "Coming to Distribution Based Similarity:\n",
    "    \n",
    "    res_similarity is not good as it is not scaled between 0-1 and for the same words,\n",
    "    Ex: sim(professor-professor) = 10.212 and sim(lunch-lunch)= 10.371255\n",
    "    \n",
    "    lin_similarity seems to be better because it captures not only the commonality but also differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
